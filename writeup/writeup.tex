\documentclass[10pt, twocolumn, twoside]{article}

\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{enumerate}
\usepackage{titleps}
\usepackage[top=0.75in,bottom=0.75in,right=0.5in,left=0.5in]{geometry}
\usepackage[parfill]{parskip}
\usepackage{titling}
\usepackage{hyperref}

\graphicspath{ {../images/} }

\newpagestyle{ruled}
{\setfoot{}{\thepage}{} \footrule}
\pagestyle{ruled}


\setlength{\droptitle}{-4em}   % This is your set screw
\posttitle{\par\end{center}\vskip 0.5em}


\title{6.867 Final Project: Detecting Facial Expressions using CNNs and Facial Landmarks} % Please change this
\date{}
\author {Vickie Ye and Alexandr Wang}


\begin{document}
\maketitle


\begin{abstract}
In this project, we compared different methods for facial expression recognition
using images from a Kaggle dataset released as a part of an ICML-2013 workshop
on representation learning.
We found that classification using features extracted manually from facial images
using principal component analysis yielded on average 40\% classification accuracy.
Using features extracted by facial landmark detection, we received on average 52\%
classification accuracy. However, when we used a convolutional neural network, we
received 65\% classification accuracy.
\end{abstract}

\section{Introduction}

In this project, we compared classification using features manually extracted from
facial images against classification using a convolutional neural net, which learns
the significant features of images through convolutional and pooling neural layers.
The two manual feature extraction methods we explored were principal component
analysis (PCA) and facial landmark detection. These extracted features were then
classified using a kernel support vector machine (SVM) and a neural network.

\subsection{PCA}

In facial recognition, PCA is used in generating a low-dimensional representation of
face images as linear combinations of the first $k$ eigenfaces of training data.
Eigenfaces are the eigenvectors of the training data's covariance matrix; the first
eigenfaces are the vectors along which the training data shows the highest variance.
Thus, we can express a facial image vector of thousands of pixels in terms of
the linearly independent basis of the first $k$ eigenvectors. 

\subsection{Facial Landmark Detection}

Facial landmarks can be extracted from facial images into lower-dimensional feature
vectors. The implementation that is used in this project uses the approach described
in \cite{Kazemi}. Kazemi et. al. uses localized histograms of gradient orientation in
a cascade of regressors that minimize squared error, in which each regressor is learned
through gradient boosting. This approach is robust to geometric and photometric
transformations, and showed less than 5\% error on the LFPW dataset.

The facial landmarks (eyes, eyebrows, nose, mouth) are intuitively the most expressive 
features in a face, and could also serve as good features for emotion classification.

\subsection{Multiclass Classification}

Support vector machines are widely used in classification problems, and is an
optimization problem that can be solved in its dual form,
\begin{align*}
\label{softsvm}
\max_{\alpha \in \mathbb{R}^n} \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_i\sum_j
\alpha_i\alpha_j y^{(i)}y^{(j)}K(x^{(i)}, x^{(j)}) & \\
\text{s.t. } 0 \le \alpha_i \le C & \\
\sum_i \alpha_i y^{(i)} = 0 &
\end{align*}
where $C$ parameterizes the soft-margin cost function, and $K(x^{(i)}, x^{(j)})$
is our kernel function, which allows us to model nonlinear classifiers.
For our multiclass classification problem, we use the nonlinear radial basis
kernel function,
\begin{equation*}
K(x, z) = \exp{(\gamma(x - z)^2)}
\end{equation*}
where $\gamma$ is a free parameter. The performance of the kernalized SVM is
highly dependent on these free parameters $C$ and $\gamma$.

Neural networks are also widely used in multiclass classification problems,
because hidden layers with nonlinear activation functions can be made to model
nonlinear classifiers well. Here, the number of nodes in each hidden layer, as
well as the number of hidden layers in each network, highly determines the accuracy
of predictions. In this project, we explored the performance of both methods.

\subsection{Convolutional Neural Network}

Convolutional neural networks (CNNs) are artificial neural networks that work particularly well for image data. The structure of CNNs exploits strong local correlation in the inputs. This is done by enforcing local connectivity between neurons of adjacent layers. The inputs of a hidden unit at a particular layer $n$ are some locally-connected subset of the units of the previous layer $n-1$, done in such a way that the input to layer $n$ represent some tile of the units of layer $n-1$, where the tiles are overlapping.

In addition, CNNs utilize shared parameterizations (weight vector and bias) across layers. By constraining the same types of weights across layers, essentially replicating units across layers, allows for features to be detected regardless to their position in the initial input, making the network much more robust to real world image data. Additionally, by constraining multiple weights to be the same reduces the parameters to be learnt, increasing learning efficiency.

\section{Experimental Details}

\subsection{Datasets}

For this project, we primarily used a dataset released by Kaggle as a part of an
ICML-2013 workshop in representation learning. This dataset included 28,709 labeled
training images, and two labeled test sets of 3,589 images.

\subsection{PCA}
We used SciPy's implementation of PCA to extract eigenfaces from our data.
In our experiments, we optimized the number of principal components $k$ we took as
our eigenface basis. Our features were then the orthogonal projection of each image
onto this basis. When performing PCA, we added whitening to ensure that eigenfaces
were uncorrelated.

\subsection{Facial Landmarks}
We used the open source implementation of \cite{Kazemi}, DLib, to detect facial
landmarks in our images. We detected 68 notable points for each facial image, which
we then used as features for our classification. The landmarks found for each image
are shown in Figure ~\ref{fig:landmarks}.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\textwidth]{landmarks}
\caption{\label{fig:landmarks} The first 12 test images with labeled facial landmarks
as generated from DLib's shape recognizer.}
\end{figure}

\subsection{Multiclass Classification}

We used SVM with a radial basis kernel to classify our manually
extracted features. We used SciPy's implementation of SVM for these purposes
To optimize the performance of SVM, we performed grid
search cross-validation on the training and validation data to optimize the
free parameters $C$ and $\gamma$ in our model.

We also used neural networks with one to two hidden layers to classify
our features. We used Google's TensorFlow neural network library for these
purposes. For both one-layer and two-layer neural networks, we performed 
model selection on the number of hidden nodes and learning rate to optimize
the final performance.

\subsection{Convolutional Neural Network}

To implement our convolutional neural network, we used Google's TensorFlow neural network library.
Our model follows the architecture described in \cite{Krizhevsky}, with a few differences in the top few layers.
In particular, we use fully connected layers instead of locally connected layers. The model described in
\cite{Krizhevsky} was designed for ImageNet's object classification, but it translated well to our problem of classifying facial expressions.

\subsubsection{Network Structure}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=\linewidth]{inference_graph}
	\caption{TensorFlow computation graph for the neural network in the CNN we implemented, illustrating each of the layers and computation done in between each layer.}
	\label{fig:inference}
\end{figure}

We implemented a deep CNN with five layers, which are from front to back: \texttt{conv1}, \texttt{conv2}, \texttt{local3}, \texttt{local4}, and \texttt{softmax\_linear}. This structure is illustrated in Figure \ref{fig:inference}. 

The layers \texttt{conv1} and \texttt{conv2} are convolution layers, which divides the image into overlapping tiles and use those as inputs to the next layer. In detail, the expression is
\[ y(i, j) = \sum_{di, dj} x(s_1i + di, s_2j + dj) f(di, dj)\]
where $y(\cdot, \cdot)$ is the output, $di$ and $dj$ are changes which range over the tile size, $s_1$ and $s_2$ are strides which measure how much you move between each convolution window across each dimension, $x(\cdot, \cdot)$ is the input, and $f(\cdot, \cdot)$ are filters, which are the weights for the layer. We didn't employ any regularization on these convolution layers, and we used window sizes of 5 by 5 pixels with a strides of 1.

As illustrated in Figure \ref{fig:inference}, after each convolution layer, we performed a \texttt{pool1} operation and a \texttt{norm1} operation. The \texttt{pool1} operation is max pooling, which will transform the input to take the maxima in tiles similarly to the convolution step. In detail,
\[ y(i, j) = \max_{di, dj} x(s_1i + di, s_2j + dj)\]
with each term defined as before. Note that this is not a layer because there are no weights. The \texttt{norm1} operation is a local response normalization response, where each input is normalized by dividing it by the squared sum of inputs within a certain radius of it. This technique is described in more detail in \cite{Krizhevsky}, and serves as form of ``brightness normalization" on the input.

The \texttt{local3} and \texttt{local4} are fully connected layers with rectified linear activation according to $f(x) = \max(x, 0)$. We employ regularization on both of these layers by adding the L2-loss of weights on both layers to our total loss function. This is especially important because they are fully connected layers, which are very prone to overfitting.

Finally, the \texttt{softmax\_linear} layer is linear transformation layer that produces unnormalized logits according to a simple linear transformation based on learned weights and biases.

\subsubsection{Training}

The loss function we minimized was primarily the cross entropy between the multinoulli distribution, formed by the normalized logits outputted by our CNN, and the probability distribution that is 1 for the correct class and 0 for all other classes. The expression for cross entropy between two probability distributions $p, q$ is
\[H(p, q) = E_p[-\log q] = -\sum_x p(x)\log q(x)\]
which measures the divergence between the two discrete probability distributions. In addition, we added standard weight decay, i.e. L2-loss, for the weights of the \texttt{local3} and \texttt{local4} layers for weight regularization.

For training, we fed the model randomly distorted training examples (see subsection \ref{overfitting}) in randomly shuffled batches of 128 samples each. For each of these examples, we determined our loss based on our current inference. After each batch, we performed back propagation using gradient descent to learn the weights, and we called each time we finished a batch a step. 

Since distorting inputs on disk is computationally expensive and bottlenecks the process, we spawn 16 threads which each asynchronously generated distorted inputs and add them to a queue which the training process pulls from.

One technique which improved model accuracy by $3\%$ when implemented was using moving averages of the learned variables. During training, we calculate the moving averages of all learned variables, and during model evaluation, we substitute all learned parameters with the moving averages instead.


\subsubsection{Minimizing Overfitting} \label{overfitting}

Since the size of our training set was 28,709 labeled examples, and our neural net needed to go through many more than 28,000 examples to achieve convergence, we employed a few techniques to ensure that our convolutional neural net did not overfit to the training set. This is also a risk because our top layers were fully connected, which means that our model could easily overfit using them.

We first implemented learning rate decay on our convolutional neural net, so that the learning rate decreased as it had been trained through more examples. We used exponential decay, so that that the learning rate decayed by a factor of $0.1$ after training through $1,200,000$ examples, and we had it decay in a step change manner as is visible in Figure \ref{fig:cnnloss}. We found that the step change learning rate decay worked better than a continuous exponential decay. This could be because maintaining a high learning rate initially could ensure that the CNN was trained towards a good local optimum in fewer steps, whereas a steadily decreasing learning rate could limit the range of the neural network.

In addition, we implemented distortion of the images while training to artificially increase the size of our training set, and make our convolutional neural net more robust to slightly distorted inputs. We processed our images in a few different ways. We cropped our 48 by 48 pixel images to a 40 by 40 pixel box, centrally for model evaluation and randomly for training. Then we approximately whiten the photos, scaling the pixel values linearly so that they pixel values have a mean of 0 and a standard deviation of 1, essentially normalizing them. This ensures consistent inputs to our neural network.

For training in particular, we performed a few more distortions to artificially increase our training set. We randomly flipped the image from left to right, randomly distorted the image brightness, and randomly distorted the image contrast for each image.
These changes to randomly distort the images greatly improved the performance of the model, raising the correct classification rate from $45.4\%$ to $53.8\%$ when initially implemented.

\section{Results and Analysis}

\subsection{PCA}

We used the first $k = 150$ principal components of the training data as our
eigenface basis. We found that this many eigenfaces explained 95\% of the variance
seen in our training data. However,
we found that the features extracted from PCA were not very powerful representations
of the data. In Figure ~\ref{fig:pca}, we show the images obtained from PCA against
the original images in the test set.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\linewidth]{original_test}
\includegraphics[width=0.8\linewidth]{projected_test}
\caption{\label{fig:pca} The first twelve images in the test set projected onto the
150-dimensional eigenface basis.}
\end{figure}



We then optimized RBF SVM on the new features, and found that the optimal set of
parameters $C=5$ and $\gamma = 0.005$ yielded a test accuracy of 41\%. The model
selection grid search can be seen in Figure ~\ref{fig:grid_pca}. Here, we see that
we indeed have converged upon at least a local minimum over the ranges $C$ = \{
1, 5, 10, 50, 100, 500, 1000, 5000\} and $\gamma$ = \{5e-5, 0.0001, 0.0005, 0.001,
0.005, 0.01\}.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\textwidth]{grid_search_pca}
\caption{\label{fig:grid_pca} Cross validation on SVM with PCA features.
Test accuracy for best parameters was 41\%.}
\end{figure}

We also fed the features into our neural network for classification. We found that
although different combinations of parameters yielded different convergence behaviors,
those we tried all converged to at most accuracy of 40\%. The convergence of a
two-layer neural net with 80 hidden nodes is shown in Figure ~\ref{fig:pca_nnet}.

\begin{figure}[!htbp]
\centering
\includegraphics[width=\linewidth]{pca_nn_accuracy}
\caption{\label{fig:pca_nnet} The convergence of a two-layer neural net with 80
hidden nodes for features extracted with PCA.}
\end{figure}

The mediocre classification accuracy on features extracted using PCA is unsurprising
given the data. The features extracted were sensitive to the variance in the
individual faces of the subjects, in addition to their facial expressions. Therefore
our standard models of classification were unable to pick out the significant
features to identify facial features. To address this problem we attempted using
facial landmarks, which are more robust to variation among individual faces.

\subsection{Facial Landmarks}

The landmarks extracted from facial images are shown in Figure ~\ref{fig:landmarks}.
For RBF SVM, we performed the same model selection grid search as described above,
which can be seen in Figure ~\ref{fig:grid_landmarks}.
We found that the optimal set of parameters yielded a test accuracy of 52\%, an
improvement over the features extracted using PCA. Again we see that we have
converged upon a minimum over the ranges $C$ = \{1, 5, 10, 50, 100, 500\} and
$\gamma$ = \{1e-7, 5e-7, 1e-6, 5e-6, 1e-5, 5e-5\}.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\textwidth]{grid_search_landmarks}
\caption{\label{fig:grid_landmarks} Cross validation on SVM with facial landmarks.
Test accuracy for best parameters was 52\%.}
\end{figure}

We again fed our features into a neural network for classification, and found that
the models we tried all converged to the same value of around 50\%, albeit with
slightly different convergence behaviors. The convergence of a two-layer neural net
with 100 hidden nodes is shown in Figure ~\ref{fig:landmarks_nnet}.

\begin{figure}[!htbp]
\centering
\includegraphics[width=\linewidth]{landmarks_nn_accuracy}
\caption{\label{fig:landmarks_nnet} The convergence of a two-layer neural net with 100
hidden nodes for facial landmark features.}
\end{figure}

Here we note that although there was an increase in performance, the performance
of both methods of classification was not impressive. In an attempt to improve
the descriptiveness of our features, we normalized the landmark points with respect
to the bounding box of the face, to minimize variance among individual feature
dimensions. However, we found that this did not aid in test accuracy. We think this
is because we did not extract enough landmark points to capture the fine facial
changes between different expressions. Related to this is the quality of the
landmark detector. However, such work is beyond the scope of this project.
To improve performance, we experimented with convolution nets for both feature
extraction and classification.

\subsection{Convolutional Neural Network}

We trained our model on Google Cloud Compute server with 4 CPUs and 15GB RAM for 20 hours using a training set of 28,709 labeled examples, and tested against a test set of 7,178 labeled examples. The model ran for 24,000 steps within those 20 hours, being fed a total of $3,072,000$ distorted training examples.

Our model achieved a correct classification rate of $65.2\%$ on the 7-way classification problem, which is comparable to the $69\%$ correct classification rate of the winning model on Kaggle. Our model correctly determines the correct label within the two highest logits with $82.3\%$ accuracy, and determines the correct label within the three highest logits with $90.5\%$ accuracy.

We can see how the total loss and the loss across the layers changed along with the learning rate in Figure \ref{fig:cnnloss}. There are a few interesting patterns. First, the weight decay loss on the \texttt{local3} and \texttt{local4} layers fell very quickly (by the 5000th step) and did not decrease past that, while still decreasing the cross entropy loss. The \texttt{local4} loss even increased slightly. 

In addition, the total loss, and in particular the cross entropy loss, dropped dramatically once the learning rate dropped according to the exponential learning rate decay schedule. This makes sense, since it allows the neural net to much more finely optimize the weights on the space it settled in with the larger learning rate. Finally, at the extremely small learning rate of $0.001$, the loss converged at around $0.5$, and any additional learning in the model at this point did not increase the classification success on the test set.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=2.4in]{learning_rate}
	\includegraphics[width=2.4in]{total_loss}
	\includegraphics[width=2.4in]{cross_entropy}
	\includegraphics[width=2.4in]{local3_loss}
	\includegraphics[width=2.4in]{local4_loss}
	\caption{The total loss, cross entropy, and learning rate of the convolutional neural net over time. The x-axis is the step, and the y-axis is the value.}
	\label{fig:cnnloss}
\end{figure}

\begin{figure*}
	\centering
	\includegraphics[width=2.4in]{conv1_params}
	\includegraphics[width=2.4in]{conv2_params}
	\includegraphics[width=2.4in]{local3_params}
	\includegraphics[width=2.4in]{local4_params}
	\includegraphics[width=2.4in]{softmax_params}
	\caption{Histograms of the weights and biases of the layers of the CNN over time. The x-axis is the step, and the y-axis is the value.}
	\label{fig:params}
\end{figure*}


We can also see the change in the parameters of the model, weights and biases, and how they changed over time in Figure \ref{fig:params}. We can see that the regularized layers \texttt{local3} and \texttt{local4} had weights that quickly converged and stayed with approximately the same distribution. The weights on the non-regularized layers, i.e. \texttt{conv1}, \texttt{conv2}, and \texttt{softmax\_linear}, mostly diverged as steps went on though, but with the smaller learning rate they stopped increasing and maintained approximately the same distribution. Finally, it is interesting that the biases for layers \texttt{conv1} and \texttt{local4} were learned to be extremely skewed distributions. This indicates that the performance of the network could potentially be improved by initializing those weights at a more skewed distribution, decreasing the initial learning time.


One feature which could be implemented and would likely improve the performance of the algorithm is dropout, which could further prevent overfitting. This would randomly drop individual nodes with a certain probability $p$ at each step, and would only increase necessary training time by a factor $1/p$.



\subsection{Comparison between the two}

\begin{thebibliography}{9}
\bibitem{Lawrence}
Lawrence, S.; Giles, L.; Tsoi, A. C.; Back, A. D. (1997)
``Face Recognition: A Convolutional Neural-Network Approach"
\textit{Neural Networks, IEEE transactions on} 8 (1):98-113

\bibitem{Matsugu}
Matsugu, M.; Mori, K.; Mitari Y.; Kaneda Y. (2003)
``Subject independent facial expression recognition with robust face detection using a convolutional neural network"
\textit{Neural Networks} 16 (5):555-559

\bibitem{Kazemi}
Kazemi, V.; Sullivan, J. (2014)
``One Millisecond Face Alignment with an Ensemble of Regression Trees"
\textit{The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}

\bibitem{Krizhevsky}
Krizhevsky, A.; Sutskever; Hinton, I.; Hinton, G. (2012)
``ImageNet Classification with Deep Convolutional Neural Networks"
\textit{Advances in Neural Information Processing Systems} 25 (1):1097-1105
\end{thebibliography}


\end{document}
